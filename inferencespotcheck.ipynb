{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yChQyWBLIKGk",
        "outputId": "ee220c1d-5a12-403b-a099-92252494af35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/87.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.2/232.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes llama-stack --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "MQN9oLqYdYC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())\n",
        "\n",
        "# Set to GPU\n",
        "torch.cuda.set_device(\"cuda:0\")\n",
        "print(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "VEJNYYnsRjn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "access_token = userdata.get(\"HF_TOKEN\")\n",
        "print(access_token)"
      ],
      "metadata": {
        "id": "nv-lP-DzmfVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    device_map=\"auto\",\n",
        "    token=access_token\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    device_map=\"auto\",\n",
        "    token=access_token\n",
        ")\n",
        "\n",
        "prompt = \"Tell me about gravity\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "output = model.generate(**model_inputs)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "Y0-lDOXwMS24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spoof data\n",
        "prompts = [\n",
        "        \"Explain quantum computing in simple terms.\",\n",
        "        \"What is the history of artificial intelligence?\",\n",
        "        \"Describe the structure of DNA.\",\n",
        "        \"What is the difference between Java and Python?\",\n",
        "        \"Explain the concept of recursion in programming.\"\n",
        "    ]\n",
        "\n",
        "N_ITERATIONS = 3\n",
        "MAX_NEW_TOKENS = 20"
      ],
      "metadata": {
        "id": "lbWlImgAYrMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def benchmark_test(benchmark_prompts):\n",
        "  elapsed_times = np.array([])\n",
        "  input_token_sizes = np.array([])\n",
        "  output_token_sizes = np.array([])\n",
        "\n",
        "  for it in range(N_ITERATIONS):\n",
        "    for p in benchmark_prompts:\n",
        "      with torch.no_grad():\n",
        "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "        input_token_sizes = np.append(input_token_sizes,\n",
        "                                      len(model_inputs['input_ids'][1]))\n",
        "\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "        output = model.generate(**model_inputs, max_new_tokens=MAX_NEW_TOKENS,\n",
        "                                pad_token_id=tokenizer.eos_token_id)\n",
        "        end_time = time.time()\n",
        "\n",
        "        output_token_sizes = np.append(output_token_sizes,\n",
        "                                       len(output[0]))\n",
        "\n",
        "      elapsed_t = end_time - start_time\n",
        "      elapsed_times = np.append(elapsed_times, elapsed_t)\n",
        "  return { \"inference_times\": elapsed_times,\n",
        "           \"input_token_sizes\": input_token_sizes,\n",
        "           \"output_token_sizes\": output_token_sizes }\n"
      ],
      "metadata": {
        "id": "G6DWaDVnjqa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run benchmark in here\n",
        "benchmark_results = benchmark_test(prompts)\n",
        "print(f\"Mean inference time (sec): {benchmark_results['inference_times'].mean()}\")\n",
        "print(f\"Mean input token size: {benchmark_results['input_token_sizes'].mean()}\")\n",
        "print(f\"Mean sec/token: { (benchmark_results['inference_times'] / benchmark_results['input_token_sizes']).mean() }\")"
      ],
      "metadata": {
        "id": "TSgBXy1yNUek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hk9JV-dGlyJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}